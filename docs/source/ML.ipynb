{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c872c679-c338-4914-8755-d952d75bba41",
   "metadata": {},
   "source": [
    "# 监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c190c8-b11b-4da7-8eec-cfa711defccb",
   "metadata": {},
   "source": [
    "X->y Learn from being given the \"right answers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351e1d6-2bc3-4e12-aa71-7573faf9df75",
   "metadata": {},
   "source": [
    "## 回归Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0f1ae-3689-44a8-9f99-7e9b38a53ff2",
   "metadata": {},
   "source": [
    "从无数可能的数字预测出一个数字\n",
    "predict numbers & large number possible outputs\n",
    "房价预测\n",
    "判断垃圾邮件\n",
    "预测用户是否点击广告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bccf7-3dce-4f3e-8d72-a13082403523",
   "metadata": {},
   "source": [
    "### 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a3c2c-a07d-4830-8fbb-519f1a1f4278",
   "metadata": {},
   "source": [
    "![img.png](img.png)\n",
    "$\\hat{y}$:预测值 $y$:实际值\n",
    "$$f_{W,b}(x) = Wx+b$$\n",
    "单个变量(feature)的线性回归，需要找到对应的W和b能很好的拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5e38c-aaa0-4574-bacb-17b452ef5ebe",
   "metadata": {},
   "source": [
    "#### 重点：构建cost function(代价函数)\n",
    "\n",
    "##### 定义\n",
    "\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - f_{W,b}(x))^2 $$\n",
    "or\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$\n",
    "m:训练集数据量\n",
    "损失函数用来衡量模型预测结果与实际结果之间差异的函数。其值越小，代表模型预测结果越接近实际结果。在训练模型时，需要通过不断调整模型参数来最小化损失函数的值。\n",
    "\n",
    "##### 直觉\n",
    "\n",
    "$$minimizeJ(w,b) \\atop{w,b}$$\n",
    "Your goal is to find a model $$f_{w,b}(x) = wx + b$$, with parameters  $w,b$,\n",
    "which will accurately predict house values given an input $x$.\n",
    "The cost is a measure of how accurate the model is on the training data.\n",
    "\n",
    "```python \n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1 / (2 * m)) * cost_sum\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "``` \n",
    "\n",
    "![img_1.png](img_1.png)\n",
    "\n",
    "```python\n",
    "\n",
    "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_train = np.array([250, 300, 480, 430, 630, 730, ])\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)\n",
    "```\n",
    "\n",
    "$$f_{w}(x) = wx$$\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "w,b自变量。所以是三维图形，可以取切面最密集的点就是拟合最好的w和b\n",
    "![img_2.png](img_2.png)\n",
    "\n",
    "### 梯度下降算法Gradient descent algorithm\n",
    "\n",
    "本质上是通过对当前$w,b$所在的位置进行修正，找到(1)式最低点的算法。  \n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing\n",
    "a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$.\n",
    "The measure is called the $cost$, $J(w,b)$.\n",
    "In training, you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "重复直到收敛\n",
    "$$\\begin{align*} \\lbrace \\newline\n",
    "\\; w &= w - \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3} \\; \\newline\n",
    "b &= b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "$\\alpha$：0,1之间。代表梯度下降的速率(learning rate)。越大越激进\n",
    "\n",
    "1.重复更新w,b的值直到算法收敛，即w,b每次更新都不会发生太大变化了\n",
    "\n",
    "2.$w,b$同时更新，异步更新可能会造成问题\n",
    "\n",
    "Where parameters$w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}$$\n",
    "![img_3.png](img_3.png)\n",
    "![img_4.png](img_4.png)\n",
    "3.越接近local minimum\n",
    "-  $$\\frac{\\partial J(w,b)}{\\partial w}$$Derivative becomes smaller\n",
    "- $\\delta{w}$Update steps become smaller\n",
    "$\\alpha$过大或过小造成的影响\n",
    "过大：达不到最低点，无法收敛(converge) 甚至发散(diverge)\n",
    "过小：步骤多且耗时\n",
    "![img_5.png](img_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22ae42-9e5d-43fe-a246-f29c5c8054b4",
   "metadata": {},
   "source": [
    "### 多元线性回归Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d3218-b479-4e24-b8c3-c0fbf7282d20",
   "metadata": {},
   "source": [
    "#### 向量化Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef422fb9-4800-4573-9fc3-474dd86ecf86",
   "metadata": {},
   "source": [
    "向量化:np.dot(w,x) + b\n",
    "No 向量化(循环计算):$$f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b$$\n",
    "\n",
    "\n",
    "![img_6.png](img_6.png)\n",
    "使用向量计算dot而非循环计算的好处：利用并行计算提升效率\n",
    "\n",
    "![img_7.gif](img_7.gif)\n",
    "<center>点乘过程</center>\n",
    "\n",
    "![img_8.png](img_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02fcb0-ff4e-4999-8666-9d9f934c5369",
   "metadata": {},
   "source": [
    "#### 多类特征Multiple features\n",
    "单个特征—>多个特征:$x->x_1,x_2,...$  \n",
    "占地面积->占地面积，卧室数量...  \n",
    "使用list存储  \n",
    "![](img_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e1963-c0c4-4f52-b13e-a277c350b2e6",
   "metadata": {},
   "source": [
    "examples are stored in a NumPy matrix X_train. Each row of the matrix represents one example. When you have $m$\n",
    "training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$\n",
    "is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_\n",
    "{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the\n",
    "subscript represents an element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa8d70-6f30-4a83-bd78-0f960dcf705e",
   "metadata": {},
   "source": [
    "#### 学习率learning rate\n",
    "![](img_10.png)\n",
    "![](img_11.png)\n",
    "\n",
    "$\\alpha$控制了参数更新的速率\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)\n",
    "\n",
    "提高学习率$\\alpha$和增加迭代次数，作用都不大，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0c3af-4fff-4df9-acb3-491f9d8f8ac0",
   "metadata": {},
   "source": [
    "#### 特征缩放Feature Scaling\n",
    "        重新缩放数据集，使特征具有相似的范围。标准化特征，让特征分布均匀。\n",
    "![](img_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba50350-265a-4373-87a2-8b39b69685b6",
   "metadata": {},
   "source": [
    "##### 如何发现特征需要被缩放\n",
    "![](img_15.png)\n",
    "\n",
    "如上图，$w_0$只需要10次迭代就收敛了，而其他参数需要迭代非常多次  \n",
    "造成这个现象的原因是$x_0$(平方英寸，千级别)非常的大，是别的feature的上千倍，所以乘以相同的倍数，该feature迭代梯度远大于其他，速度更快。  \n",
    "上图展示了w更新不均匀的原因。   \n",
    "  $\\alpha$由所有参数更新（w和b）共享。  \n",
    "  常见错误项乘以$w$的特征。  \n",
    "  特征的大小差异很大，使得一些特征的更新速度比其他特征快得多。在这种情况下，$w_0$乘以“大小（平方英尺）”，通常>1000，而w_1乘以“卧室数量”，通常为2-4。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59199dca-c2c4-47f9-b2fb-ce9384a5e573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:32:31.812157Z",
     "iopub.status.busy": "2025-01-01T13:32:31.811554Z",
     "iopub.status.idle": "2025-01-01T13:32:31.821368Z",
     "shell.execute_reply": "2025-01-01T13:32:31.820266Z",
     "shell.execute_reply.started": "2025-01-01T13:32:31.812108Z"
    }
   },
   "source": [
    "##### 三种技巧\n",
    "\n",
    "- 特征缩放, 让每个特征除以人为规定的值，结果落在-1和1之间。\n",
    "- 均值归一化Mean normalization: $$x_i := \\dfrac{x_i - \\mu_i}{max - min} $$ \n",
    "- z-score归一化 Z-score normalization，所有特征的均值为0，标准差为1。\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag 4$$\n",
    "where $j$ selects a feature or a column in the X matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934399-648a-4cfd-828c-9dbe0966ebc4",
   "metadata": {},
   "source": [
    "#### 多元线性回归model\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "  or in vector notation:\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
    "  where $$\\cdot$$ is a vector dot product(向量乘)\n",
    "  To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5426937-341f-417f-8d49-1421bd6a9deb",
   "metadata": {},
   "source": [
    "#### 线性回归的评价指标\n",
    "\n",
    "**均方误差（Mean Squared Error，MSE）**：\n",
    "\n",
    "MSE 是模型预测值与真实值差的平方的平均值。它强调大的误差。\n",
    "\n",
    "$$\\begin{align}MSE = \\frac{\\Sigma (y_i - \\hat{y}_i)^2}{n}\\end{align}$$\n",
    "\n",
    "适用场景：适用于回归任务中，尤其是对大的预测误差更加敏感的场景。\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e557a3-9777-4d74-a369-4fd20cc25a7b",
   "metadata": {},
   "source": [
    "    \n",
    "**均方根误差（Root Mean Squared Error，RMSE）**：\n",
    "\n",
    "RMSE 是 MSE 的平方根，用于将误差带回与原始目标变量相同的量纲\n",
    "\n",
    "$$\\begin{align} RMSE = \\sqrt{\\frac{\\Sigma (y_i - \\hat{y}_i)^2}{n}} \\end{align}$$\n",
    "\n",
    "适用场景：与 MSE 类似，但 RMSE 更直观，误差与目标变量的尺度一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c44047-ce29-496f-a32e-403a1daa9138",
   "metadata": {},
   "source": [
    "**平均绝对误差（Mean Absolute Error，MAE）**：\n",
    "\n",
    "MAE 是预测值与真实值的绝对差的平均值，较少受大误差的影响。\n",
    "\n",
    "$$\\begin{align}MAE = \\frac{\\Sigma |y_i - \\hat{y}_i|}{n}\\end{align}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615982f0-32ba-4df7-ae5e-9b0185191017",
   "metadata": {},
   "source": [
    "**决定系数（Coefficient of Determination，R-squared，R²）**：\n",
    "R²度量了线性回归模型对数据的拟合程度。它的取值范围在0到1之间，越接近1表示模型拟合得越好。\n",
    "适用场景：用于评估回归模型的解释能力。\n",
    "\n",
    "$$\\begin{align}R² = 1 - \\frac{\\Sigma (yi - ŷi)²}{ \\Sigma (yi - ȳ)²}\\end{align}$$\n",
    "其中 yi 表示实际观测值，ŷi 表示模型的预测值，ȳ 表示实际观测值的均值。\n",
    "对于线性回归模型来说，除了SSE以外，我们还可使用决定系数（R-square，也被称为拟合优度检验）作为其模型评估指标。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948e64a8-6458-4e85-92ad-8477e04f9fbd",
   "metadata": {},
   "source": [
    "决定系数的计算需要使用之前介绍的组间误差平方和和离差平方和的概念。在回归分析中，SSR表示聚类中类似的组间平方和概念，表意为Sum of squares of the regression，由预测数据与标签均值之间差值的平方和计算得出：\n",
    "\n",
    "预测与均值的距离：\n",
    "  $$SSR =\\sum^{n}_{i=1}(\\bar{y_i}-\\hat{y_i})^2$$\n",
    "\n",
    "实际与均值的距离(方差):\n",
    "$$SST =\\sum^{n}_{i=1}(\\bar{y_i}-y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090fcdd-ad23-4a87-bfe2-22f3880bdca7",
   "metadata": {},
   "source": [
    "并且，$SST$可由$$SSR+SSE$$计算得出。而决定系数，则由$SSR$和$SST$共同决定：\n",
    "$$R-square=\\frac{SSR}{SST}=\\frac{SST-SSE}{SSE}=1-\\frac{SSE}{SST}$$\n",
    "很明显，决定系数是一个鉴于[0,1]之间的值，并且约趋近于1，模型拟合效果越好。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbb86f2-2b74-491f-81fe-1e97f2743772",
   "metadata": {},
   "source": [
    "**调整R方值**\n",
    "\n",
    "调整后的R方值（Adjusted R²）是在多个自变量的回归分析中更准确地评估模型的拟合优度。与普通R方值不同，调整后的R²会考虑自变量数量，不会因为增加不相关的自变量而人为提高模型的拟合度。\n",
    "$$\\begin{align} \\text{Adjusted } R² = 1 - \\left(\\frac{(1 - R²)(n - 1)}{n - k - 1}\\right) \\end{align}$$其中： $R²$是普通的决定系数；$n$ 是样本数量； $k$ 是自变量的数量。\n",
    "适用场景：在特征数较多时，调整 R² 比普通 R² 更合理。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ea4a5d7-091c-4582-992a-3a6c81a315d9",
   "metadata": {},
   "source": [
    "## 分类Classification\n",
    "\n",
    "predict categories & small number of out puts\n",
    "\n",
    "Binary classification：$y$只能是0和1的其中一个\n",
    "\n",
    "根据肿瘤大小判断是否癌症"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8676eef-8b46-4fdf-bc77-5263e23b1cc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-02T15:03:37.800491Z",
     "iopub.status.busy": "2025-01-02T15:03:37.800074Z",
     "iopub.status.idle": "2025-01-02T15:03:37.810489Z",
     "shell.execute_reply": "2025-01-02T15:03:37.805246Z",
     "shell.execute_reply.started": "2025-01-02T15:03:37.800435Z"
    }
   },
   "source": [
    "### Sigmod or logistic函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a26cb-b62a-4a32-a099-a368b8028af5",
   "metadata": {},
   "source": [
    "肿瘤大小(横轴)判定是否癌症(0,1纵轴)，数据分布如图。可以拟合成 曲线，即sigmod函数。\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}\\tag{1}$$\n",
    "\n",
    "![](img_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79564e7-ac89-407e-ba21-f6ef08ce7f12",
   "metadata": {},
   "source": [
    "在逻辑回归的情况下，z（sigmoid函数的输入）是线性回归模型的输出。 \n",
    "- 在单个示例的情况下，z是标量。\n",
    "- 在多个示例的情况下，z可以是由m个值组成的向量，每个示例一个。 \n",
    "- sigmoid函数的实现应该涵盖这两种潜在的输入格式。\n",
    "\n",
    "让我们在Python中实现这一点。\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    \"\"\"6\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A scalar, numpy array of any size.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z\n",
    "         \n",
    "    \"\"\"\n",
    "\n",
    "    g = 1/(1+np.exp(-z))\n",
    "   \n",
    "    return g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce3eeb-8d91-4183-921c-f5ca7966d161",
   "metadata": {},
   "source": [
    "### 逻辑回归Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d72cc6c-001e-4349-a0e1-bfac913fe22d",
   "metadata": {},
   "source": [
    "A logistic regression model applies the sigmoid to the familiar linear regression model as shown below:\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b ) \\tag{2} $$\n",
    "  where$$g(z) = \\frac{1}{1+e^{-z}}\\tag{3}$$\n",
    "Final formula\n",
    "$$f_{w,b}(x^{(i)}) = \\frac1{1 + e^{-wx+b}}\\tag{4}$$\n",
    "输出结果$$f_{w,b}(x^{(i)}) = 0.7$$,\n",
    "代表患癌概率为70%\n",
    "\n",
    "\n",
    "![](img_17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba6c74d-42d9-46bc-b69d-550988209e1f",
   "metadata": {},
   "source": [
    "#### 损失函数Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ee67d-2de1-43c8-ad68-239037770720",
   "metadata": {},
   "source": [
    "Loss：单个示例与目标值之间差异的指标\n",
    "\n",
    "Cost：训练集损失的衡量标准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44161ed7-3b72-41b9-a80a-f7e80ef04bed",
   "metadata": {},
   "source": [
    "会导致结果出现多个局部最小值。算法确定全局最小值较为困难\n",
    "      $$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$的梯度cost曲线\n",
    "\n",
    "![](img_18.png)\n",
    "      \n",
    "![](img_19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0cdb8-058e-4563-b474-aa75ae92338a",
   "metadata": {},
   "source": [
    "方法：离散数据连续化\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$$ 单个示例的cost, 如下:\n",
    "$$\\begin{equation}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = \\begin{cases}\n",
    "- \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=1$}\\\\\n",
    "\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) & \\text{if $y^{(i)}=0$}\n",
    "\\end{cases}\n",
    "\\end{equation}$$\n",
    "      $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$$ 是预测值, while $y^{(i)}$ 是目标值.\n",
    "      $$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot\\mathbf{x}^{(i)}+b)$$  $g$ 是 sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd9e624-3104-46da-a6c7-12f085f272a6",
   "metadata": {},
   "source": [
    "该损失函数由两条曲线构成，目标值为1和0时曲线的表现不同。越接近目标，曲线斜率越小，即损失函数对参数更不敏感。\"速度放缓说明越接近真理\",如图:\n",
    "\n",
    "![](img_20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf12ee7-7a2f-4c44-a0a6-fd2562e7e592",
   "metadata": {},
   "source": [
    "#### 损失函数简化版\n",
    "(1)式等价于\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "该式中 $$y^{(i)} = 1 \\space or \\space0$$\n",
    "$$\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 0) &= (-(0) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 0\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\\\\n",
    "&= -\\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}$$\n",
    "$$\\begin{align}\n",
    "loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), 1) &=  (-(1) \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - 1\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\\\\\n",
    "&=  -\\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcc8c3-74b2-4786-aeb2-18a2fa7d5ecc",
   "metadata": {},
   "source": [
    "下图成本与参数曲线:\n",
    "```python\n",
    "plt.close('all')\n",
    "cst = plt_logistic_cost(x_train,y_train)\n",
    "```\n",
    "![](img_21.png)\n",
    "\n",
    "这条曲线非常适合梯度下降。它没有停滞点(plateau)、局部极小值或不连续点。注意，它不像平方误差那样是碗状的。绘制成本和成本的对数是为了说明这样一个事实，即当成本较小时，曲线有一个斜率并继续下降。\n",
    "假设使用线性回归损失函数模拟\n",
    "单示例下成本函数为:\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \\tag{1}$$\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a342c-ae4c-4008-b6d9-5460563f34eb",
   "metadata": {},
   "source": [
    "#### 代价函数Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa90300-8d4f-4b86-9211-cd223fd617b2",
   "metadata": {},
   "source": [
    "将所有的loss组合起来，成为成本函数\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "- $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$是单个数据点的cost\n",
    "$$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "- m是数据集中训练的样本量:\n",
    "$$\\begin{align}\n",
    "f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed5f058-f205-443f-811f-e8807826d46c",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba1934-04e5-48b9-b0d4-503f7450ff9a",
   "metadata": {},
   "source": [
    "## 偏度（skewness）\n",
    "有偏度的数据不利于我们的建模预测。原因是很多模型都**假设数据误差项符合正态分布**  \n",
    "数据集偏离平均值的概率。\n",
    "\n",
    "是用来衡量概率分布或数据集中不对称程度的统计量。它描述了数据分布的尾部（tail）在平均值的哪一侧更重或更长。  \n",
    "偏度可以帮助我们了解数据的**偏斜性质**，即**数据相对于平均值的分布情况**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf88c8-6dda-43ea-bbfc-fab9bf6a5eb3",
   "metadata": {},
   "source": [
    "右偏:右偏分布在其峰值的右侧比其左侧更长。右偏也被称为正偏。它表明在分布的极端一端有观测值，但它们相对较少。右偏分布的右侧有一条长尾。\n",
    "\n",
    "左偏:左偏分布的峰值左侧比右侧更长。左偏分布的左侧有一条长尾。左偏也被称为负偏。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bde58c-23de-4c47-ab9b-0170367fe1e3",
   "metadata": {},
   "source": [
    "### 现象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71120cc0-7f49-45e0-a5d7-6a6a4ef0c7e8",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cb465d-e17c-4ab3-839b-78f0070280af",
   "metadata": {},
   "source": [
    "在预测sticker销量比赛中，遇到了右偏程度比较大的标签。大部分销量集中[0, 1000]。  \n",
    "对标签标准化前后，同样的参数在lightgbm的表现相差了至少30%  \n",
    "另外一个有趣的现象是，在使用xgboost的gamma回归器时能显著降低训练集的mape,这是因为这个数据符合gamama分布，但是并不能提高拟合效果\n",
    "| Syntax      | 标准化后 | 标准化前 |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Overall Train MAPE      | 0.0457       |0.5495|\n",
    "| Overall OOF MAPE   | 0.0859        |0.5801| \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "631d43d5-4d9f-41df-b0d7-fe48349c8bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T04:40:29.379303Z",
     "iopub.status.busy": "2025-01-20T04:40:29.378974Z",
     "iopub.status.idle": "2025-01-20T04:43:44.949795Z",
     "shell.execute_reply": "2025-01-20T04:43:44.948583Z",
     "shell.execute_reply.started": "2025-01-20T04:40:29.379274Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [03:15<00:00, 39.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Train MAPE: 0.0457\n",
      "Overall OOF MAPE: 0.0859 \n",
      "CPU times: user 12min 37s, sys: 2.08 s, total: 12min 39s\n",
      "Wall time: 3min 15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params =  {'n_estimators': 2203, 'max_depth': 5, 'colsample_bytree': 0.5359752614980476,\n",
    "            'subsample': 0.7271274739921461, 'learning_rate': 0.011247656752870117, 'min_child_weight': 74}\n",
    "\n",
    "XGBresult = base.Train_ML(params,'XGB', y_log=True, g_col='year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "babd0014-37a5-4437-ad2d-920d8794b65f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-20T04:43:44.951293Z",
     "iopub.status.busy": "2025-01-20T04:43:44.950867Z",
     "iopub.status.idle": "2025-01-20T04:46:59.657337Z",
     "shell.execute_reply": "2025-01-20T04:46:59.655994Z",
     "shell.execute_reply.started": "2025-01-20T04:43:44.951251Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [03:14<00:00, 38.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Train MAPE: 0.5495\n",
      "Overall OOF MAPE: 0.5801 \n",
      "CPU times: user 12min 28s, sys: 1.95 s, total: 12min 30s\n",
      "Wall time: 3min 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params =  {'n_estimators': 2203, 'max_depth': 5, 'colsample_bytree': 0.5359752614980476,\n",
    "            'subsample': 0.7271274739921461, 'learning_rate': 0.011247656752870117, 'min_child_weight': 74}\n",
    "\n",
    "XGBresult = base.Train_ML(params,'XGB', y_log=False, g_col='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffe2ba-efeb-410c-acbc-5433bca26f99",
   "metadata": {},
   "source": [
    "### 处理方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e9782f-d0f9-4a1c-b6f1-29e739520963",
   "metadata": {},
   "source": [
    "在上面的案例中，使用了log10(x+1)，来处理偏度较大的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b39b5-697d-47cd-9579-4680403a961f",
   "metadata": {},
   "source": [
    "如何处理有偏度数据\n",
    "\n",
    "如果你的统计过程需要正态分布并且你的数据是倾斜的，你通常有三个选择:\n",
    "\n",
    "- 什么也不做：许多统计检验，包括t检验、方差分析和线性回归，对偏斜数据不太敏感。特别是如果偏斜是轻微或中度的，最好的办法就是忽略它。\n",
    "\n",
    "- 数据转换：通过对数据应用某种变换，可以调整数据的分布形状，使其更接近**对称分布**。常见的数据转换方法包括取对数、开方、平方根等。这些转换可以减小或消除数据的偏度。\n",
    "\n",
    "- 使用不同的模型：你可能想选择一个不假设正态分布的模型，非参数测试或广义线性模型可能更适合您的数据。比如说非参数方法：如果数据的偏度较大，而且无法通过简单的转换来纠正，可以考虑使用非参数统计方法。非参数方法不依赖于分布的假设，而是直接对数据进行分析，例如使用中位数作为代表性的位置测度，而不是平均值。\n",
    "\n",
    "- 分组分析：如果数据集中存在明显的子群体，可以考虑对数据进行分组分析。通过将数据分成多个子群体，并对每个子群体进行单独的分析，可以更好地了解数据的特征和偏度情况。\n",
    "\n",
    "- 针对特定问题采取相应的方法：根据具体的数据和分析目的，可以采用特定的方法来处理偏度数据。例如，在回归分析中，可以使用偏度稳定转换（skewness-stabilizing transformation）来调整数据的偏度，以满足回归模型的假设。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8fa3b8-a1a4-4931-af28-bd97ee8d7a03",
   "metadata": {},
   "source": [
    "**根据偏态类型的变换**\n",
    "\n",
    "| 偏态类型   | 偏态强度     | 变换方法            |\n",
    "| ---------- | ------------ | ------------------- |\n",
    "| **右偏**   | 轻微偏态     | 不变换              |\n",
    "|            | 中度偏态     | 开方                |\n",
    "|            | 强偏态       | 自然对数            |\n",
    "|            | 非常强偏态   | 以10为底的对数      |\n",
    "| **左偏**   | 轻微偏态     | 不变换              |\n",
    "|            | 中度偏态     | 反转*然后开方       |\n",
    "|            | 强偏态       | 反转*然后自然对数   |\n",
    "|            | 非常强偏态   | 反转*然后以10为底的对数 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
