{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c872c679-c338-4914-8755-d952d75bba41",
   "metadata": {},
   "source": [
    "# 监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c190c8-b11b-4da7-8eec-cfa711defccb",
   "metadata": {},
   "source": [
    "X->y Learn from being given the \"right answers\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351e1d6-2bc3-4e12-aa71-7573faf9df75",
   "metadata": {},
   "source": [
    "## 回归Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb0f1ae-3689-44a8-9f99-7e9b38a53ff2",
   "metadata": {},
   "source": [
    "从无数可能的数字预测出一个数字\n",
    "predict numbers & large number possible outputs\n",
    "房价预测\n",
    "判断垃圾邮件\n",
    "预测用户是否点击广告"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190bccf7-3dce-4f3e-8d72-a13082403523",
   "metadata": {},
   "source": [
    "### 线性回归模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356a3c2c-a07d-4830-8fbb-519f1a1f4278",
   "metadata": {},
   "source": [
    "![img.png](img.png)\n",
    "$\\hat{y}$:预测值 $y$:实际值\n",
    "$$f_{W,b}(x) = Wx+b$$\n",
    "单个变量(feature)的线性回归，需要找到对应的W和b能很好的拟合数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea5e38c-aaa0-4574-bacb-17b452ef5ebe",
   "metadata": {},
   "source": [
    "#### 重点：构建cost function(代价函数)\n",
    "\n",
    "##### 定义\n",
    "\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - f_{W,b}(x))^2 $$\n",
    "or\n",
    "$$J{(w,b)} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$\n",
    "m:训练集数据量\n",
    "损失函数用来衡量模型预测结果与实际结果之间差异的函数。其值越小，代表模型预测结果越接近实际结果。在训练模型时，需要通过不断调整模型参数来最小化损失函数的值。\n",
    "\n",
    "##### 直觉\n",
    "\n",
    "$$minimizeJ(w,b) \\atop{w,b}$$\n",
    "Your goal is to find a model $$f_{w,b}(x) = wx + b$$, with parameters  $w,b$,\n",
    "which will accurately predict house values given an input $x$.\n",
    "The cost is a measure of how accurate the model is on the training data.\n",
    "\n",
    "```python \n",
    "def compute_cost(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost function for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    \n",
    "    Returns\n",
    "        total_cost (float): The cost of using w,b as the parameters for linear regression\n",
    "               to fit the data points in x and y\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m = x.shape[0]\n",
    "    cost_sum = 0\n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b\n",
    "        cost = (f_wb - y[i]) ** 2\n",
    "        cost_sum = cost_sum + cost\n",
    "    total_cost = (1 / (2 * m)) * cost_sum\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "``` \n",
    "\n",
    "![img_1.png](img_1.png)\n",
    "\n",
    "```python\n",
    "\n",
    "x_train = np.array([1.0, 1.7, 2.0, 2.5, 3.0, 3.2])\n",
    "y_train = np.array([250, 300, 480, 430, 630, 730, ])\n",
    "\n",
    "plt.close('all')\n",
    "fig, ax, dyn_items = plt_stationary(x_train, y_train)\n",
    "updater = plt_update_onclick(fig, ax, x_train, y_train, dyn_items)\n",
    "```\n",
    "\n",
    "$$f_{w}(x) = wx$$\n",
    "\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "w,b自变量。所以是三维图形，可以取切面最密集的点就是拟合最好的w和b\n",
    "![img_2.png](img_2.png)\n",
    "\n",
    "### 梯度下降算法Gradient descent algorithm\n",
    "\n",
    "本质上是通过对当前$w,b$所在的位置进行修正，找到(1)式最低点的算法。  \n",
    "So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing\n",
    "a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$.\n",
    "The measure is called the $cost$, $J(w,b)$.\n",
    "In training, you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n",
    "重复直到收敛\n",
    "$$\\begin{align*} \\lbrace \\newline\n",
    "\\; w &= w - \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3} \\; \\newline\n",
    "b &= b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "$\\alpha$：0,1之间。代表梯度下降的速率(learning rate)。越大越激进\n",
    "\n",
    "1.重复更新w,b的值直到算法收敛，即w,b每次更新都不会发生太大变化了\n",
    "\n",
    "2.$w,b$同时更新，异步更新可能会造成问题\n",
    "\n",
    "Where parameters$w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "\\frac{\\partial J(w,b)}{\\partial b} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}$$\n",
    "![img_3.png](img_3.png)\n",
    "![img_4.png](img_4.png)\n",
    "3.越接近local minimum\n",
    "-  $$\\frac{\\partial J(w,b)}{\\partial w}$$Derivative becomes smaller\n",
    "- $\\delta{w}$Update steps become smaller\n",
    "$\\alpha$过大或过小造成的影响\n",
    "过大：达不到最低点，无法收敛(converge) 甚至发散(diverge)\n",
    "过小：步骤多且耗时\n",
    "![img_5.png](img_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a22ae42-9e5d-43fe-a246-f29c5c8054b4",
   "metadata": {},
   "source": [
    "### 多元线性回归Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757d3218-b479-4e24-b8c3-c0fbf7282d20",
   "metadata": {},
   "source": [
    "#### 向量化Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef422fb9-4800-4573-9fc3-474dd86ecf86",
   "metadata": {},
   "source": [
    "向量化:np.dot(w,x) + b\n",
    "No 向量化(循环计算):$$f = w[0] * x[0] + w[1] * x[1] + w[2] * x[2] + b$$\n",
    "\n",
    "\n",
    "![img_6.png](img_6.png)\n",
    "使用向量计算dot而非循环计算的好处：利用并行计算提升效率\n",
    "\n",
    "![img_7.gif](img_7.gif)\n",
    "<center>点乘过程</center>\n",
    "\n",
    "![img_8.png](img_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc02fcb0-ff4e-4999-8666-9d9f934c5369",
   "metadata": {},
   "source": [
    "#### 多类特征Multiple features\n",
    "单个特征—>多个特征:$x->x_1,x_2,...$  \n",
    "占地面积->占地面积，卧室数量...  \n",
    "使用list存储  \n",
    "![](img_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e1963-c0c4-4f52-b13e-a277c350b2e6",
   "metadata": {},
   "source": [
    "examples are stored in a NumPy matrix X_train. Each row of the matrix represents one example. When you have $m$\n",
    "training examples ( $m$ is three in our example), and there are $n$ features (four in our example), $\\mathbf{X}$\n",
    "is a matrix with dimensions ($m$, $n$) (m rows, n columns).\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{pmatrix}\n",
    " x^{(0)}_0 & x^{(0)}_1 & \\cdots & x^{(0)}_{n-1} \\\\ \n",
    " x^{(1)}_0 & x^{(1)}_1 & \\cdots & x^{(1)}_{n-1} \\\\\n",
    " \\cdots \\\\\n",
    " x^{(m-1)}_0 & x^{(m-1)}_1 & \\cdots & x^{(m-1)}_{n-1} \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "notation:\n",
    "- $\\mathbf{x}^{(i)}$ is vector containing example i. $\\mathbf{x}^{(i)} = (x^{(i)}_0, x^{(i)}_1, \\cdots,x^{(i)}_\n",
    "{n-1})$\n",
    "- $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the\n",
    "subscript represents an element."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfa8d70-6f30-4a83-bd78-0f960dcf705e",
   "metadata": {},
   "source": [
    "#### 学习率learning rate\n",
    "![](img_10.png)\n",
    "![](img_11.png)\n",
    "\n",
    "$\\alpha$控制了参数更新的速率\n",
    "\n",
    "![](img_12.png)\n",
    "\n",
    "![](img_13.png)\n",
    "\n",
    "提高学习率$\\alpha$和增加迭代次数，作用都不大，如何解决？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a0c3af-4fff-4df9-acb3-491f9d8f8ac0",
   "metadata": {},
   "source": [
    "#### 特征缩放Feature Scaling\n",
    "        重新缩放数据集，使特征具有相似的范围。标准化特征，让特征分布均匀。\n",
    "![](img_14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba50350-265a-4373-87a2-8b39b69685b6",
   "metadata": {},
   "source": [
    "##### 如何发现特征需要被缩放\n",
    "![](img_15.png)\n",
    "\n",
    "如上图，$w_0$只需要10次迭代就收敛了，而其他参数需要迭代非常多次  \n",
    "造成这个现象的原因是$x_0$(平方英寸，千级别)非常的大，是别的feature的上千倍，所以乘以相同的倍数，该feature迭代梯度远大于其他，速度更快。  \n",
    "上图展示了w更新不均匀的原因。   \n",
    "  $\\alpha$由所有参数更新（w和b）共享。  \n",
    "  常见错误项乘以$w$的特征。  \n",
    "  特征的大小差异很大，使得一些特征的更新速度比其他特征快得多。在这种情况下，$w_0$乘以“大小（平方英尺）”，通常>1000，而w_1乘以“卧室数量”，通常为2-4。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59199dca-c2c4-47f9-b2fb-ce9384a5e573",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-01T13:32:31.812157Z",
     "iopub.status.busy": "2025-01-01T13:32:31.811554Z",
     "iopub.status.idle": "2025-01-01T13:32:31.821368Z",
     "shell.execute_reply": "2025-01-01T13:32:31.820266Z",
     "shell.execute_reply.started": "2025-01-01T13:32:31.812108Z"
    }
   },
   "source": [
    "##### 三种技巧\n",
    "\n",
    "- 特征缩放, 让每个特征除以人为规定的值，结果落在-1和1之间。\n",
    "- 均值归一化Mean normalization: $$x_i := \\dfrac{x_i - \\mu_i}{max - min} $$ \n",
    "- z-score归一化 Z-score normalization，所有特征的均值为0，标准差为1。\n",
    "$$x^{(i)}_j = \\dfrac{x^{(i)}_j - \\mu_j}{\\sigma_j} \\tag 4$$\n",
    "where $j$ selects a feature or a column in the X matrix. $µ_j$ is the mean of all the values for feature (j) and $\\sigma_j$ is the standard deviation of feature (j).\n",
    "$$\\begin{align}\n",
    "\\mu_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} x^{(i)}_j \\tag{5}\\\\\n",
    "\\sigma^2_j &= \\frac{1}{m} \\sum_{i=0}^{m-1} (x^{(i)}_j - \\mu_j)^2  \\tag{6}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b934399-648a-4cfd-828c-9dbe0966ebc4",
   "metadata": {},
   "source": [
    "#### 多元线性回归model\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "  or in vector notation:\n",
    "  $$f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$\n",
    "  where $$\\cdot$$ is a vector dot product(向量乘)\n",
    "  To demonstrate the dot product, we will implement prediction using (1) and (2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
